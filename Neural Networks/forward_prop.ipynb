{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e925385-6756-4600-bbf8-208fe17e3b1c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<p style=\"float:right;\"><i>Created By Maroyi Bisoka on 01/02/2025</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "de731ba6-a2a3-47d4-a279-07f9d897372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3855fecf-82f3-478b-97ff-442f4cab953f",
   "metadata": {},
   "source": [
    "**A dense layer**, also known as a fully connected layer, is a type of neural network layer where every neuron in the layer is connected to every neuron in the previous layer. This connectivity allows each neuron to receive input from all neurons in the preceding layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a650d6ea-f31f-473d-b426-086aec08bf35",
   "metadata": {},
   "source": [
    "<img src=\"./neural_network_img.png\" style=\"width: 400px; height: auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "225525be-454b-4ae3-a8f1-836e8d9639e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function Choosed: Sigmoid \n",
    "def sigmoid(z):\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "9c1543fb-83e4-489a-a912-f310041b96a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the activation function of each neuron within a single Layer\n",
    "def dense(a_in, W, b, activation_function):\n",
    "    no_neurons = W.shape[1]\n",
    "    a_out = np.zeros(no_neurons)\n",
    "    for j in range(no_neurons):\n",
    "        w = W[:, j]\n",
    "        z = np.dot(a_in, w) + b[j]\n",
    "        a_out[j] = activation_function(z)\n",
    "    return a_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "26b40cfe-835e-4be6-9120-f08fdbbf76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the forward propagation algorithm\n",
    "def sequential(x, W_list, b_list, activation_function):\n",
    "    no_layers = len(W_list)\n",
    "    a_in  = x\n",
    "    for i in range(no_layers):\n",
    "        a_out = dense(a_in, W_list[i], b_list[i], activation_function)\n",
    "        a_in = a_out\n",
    "    return a_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "cf178a24-6a22-4c55-b3e0-136a8edef984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 11])"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random initialization of the value of x\n",
    "np.random.seed(1)\n",
    "x = np.random.randint(0, 20, size=2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "55bc0c0d-c8ce-43b9-a8c3-136432725cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random initialization for the parameters W and b\n",
    "\n",
    "l1_shape = (2,2) # layer1_shape (number of W in a single neurons, number of neurons)\n",
    "l2_shape = (2,1) # layer2_shape (number of W in a single neurons, number of neurons)\n",
    "\n",
    "np.random.seed(10) # used to generate same random number\n",
    "W1 = np.random.rand(l1_shape[0], l1_shape[1])\n",
    "\n",
    "np.random.seed(20)\n",
    "b1 =  np.random.rand(l1_shape[1])\n",
    "\n",
    "np.random.seed(30) \n",
    "W2 = np.random.rand(l2_shape[0], l2_shape[1])\n",
    "\n",
    "np.random.seed(40) \n",
    "b2 = np.random.rand(l2_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "f37d896c-2e53-4820-8b3f-2a3631fa056d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.77132064, 0.02075195],\n",
       "        [0.63364823, 0.74880388]]),\n",
       " array([0.5881308 , 0.89771373]))"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "81047907-71b6-4619-a40e-99900cd918ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1_1:  [0.77132064 0.63364823]\n",
      "W2_1:  [0.02075195 0.74880388]\n"
     ]
    }
   ],
   "source": [
    "# W1 means all the w for layer 1\n",
    "# W1_1 means all the w for the 1st layer 1st neuron\n",
    "W1_1 = W1[:, 0]\n",
    "print('W1_1: ', W1_1)\n",
    "# W1_2 means all the w for the 1st layer 2nd neuron\n",
    "W2_1= W1[:, 1]\n",
    "print('W2_1: ', W2_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "718b750b-8765-40d5-aeaa-8ccd2e9e995c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.64414354],\n",
       "        [0.38074849]]),\n",
       " array([0.40768703]))"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "5280b6c2-ad83-4bf8-b523-d63bdd80dac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1_1:  [0.64414354 0.38074849]\n"
     ]
    }
   ],
   "source": [
    "# W2 means all the w for layer 2\n",
    "# W2_1 means all the w for the 2nd layer 1st neuron\n",
    "W1_1 = W2[:, 0]\n",
    "print('W1_1: ', W1_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2b5f2a-f763-4e4d-b49a-58878e3b2c4c",
   "metadata": {},
   "source": [
    "\n",
    "<br />\n",
    "<hr />\n",
    "\n",
    "**NB**: All of the rows of a specific col within a W matrix specify the w's of a specific neuron.\n",
    "\n",
    "For example: \n",
    "- If we're looking for the w's of the 1st layer 1st neuron --> **W1[: , 0]** we used **':'** for the row to get all the row and **'0'** for the column since python indexing start from zero (neuron 1 at index 0, neuron 2 at index 1, and so on).\n",
    "- If we're looking for the w's of the 1st layer 2nd neuron  --> **W1[: , 1]**\n",
    "- If we're looking for the w's of the 2nd layer 1st neuron  --> **W2[: , 0]**\n",
    "- If we're looking for the b of the 1st layer 2nd neuron  --> **b1[1]**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "3f81575a-292f-4472-8c20-e19754c30fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_list = [W1, W2]\n",
    "b_list = [b1, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "ee7d42c9-f927-4f99-b9bf-17d0c49a952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Forward propagation\n",
    "a_out = sequential(x, W_list, b_list, sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "95e5383b-1c73-4067-8ace-1712bf32072a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80729598])"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output Layer value\n",
    "a_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786d1629-46fd-4994-bf4f-59a25c10becd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
